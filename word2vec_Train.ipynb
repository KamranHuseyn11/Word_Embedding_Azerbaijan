{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO\n",
    "-------------------------------------------------------------\n",
    "This script uses NLTK and pyhton re to tokenize corpus and gensim lybrary to train corpus with Word2Vec altgorithm.\n",
    "you can change hyperparameters for Word2Vec training depending on your choice.\n",
    "\n",
    "------------------------------------------------------------\n",
    "After training process obtained vectors can be uploaded\n",
    "\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import gc\n",
    "\n",
    "\n",
    "file = open(_your_corpus_,'r',encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization \n",
    "WORD = re.compile(r'\\w+')\n",
    "all_words = []\n",
    "\n",
    "for sent in nltk.sent_tokenize(corpus):\n",
    "    all_words.append(WORD.findall(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec(all_words, min_count=12,size =300, window = 6, negative = 6 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After training it is a good idea to erase corpus and tokens to free up memory\n",
    "corpus = None\n",
    "all_words = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('səssizlik', 0.7219706773757935),\n",
       " ('sükutu', 0.610196590423584),\n",
       " ('sakitlik', 0.5771867036819458),\n",
       " ('sükuta', 0.5597708225250244),\n",
       " ('süküt', 0.5528272390365601),\n",
       " ('qarğaşalıq', 0.5355178713798523),\n",
       " ('şaşqınlıq', 0.5251564979553223),\n",
       " ('səksəkə', 0.523532509803772),\n",
       " ('təlaş', 0.5174703598022461),\n",
       " ('çökdü', 0.5156707763671875)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some predictions on model\n",
    "\n",
    "word2vec.wv.most_similar(positive = [\"sükut\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving vectors to txt file vectors can be later used for testing or for feature extraction for NLP tasks. \n",
    "# However It is not possible to use this file for more training, for such purposes you shoul store model itself.\n",
    "\n",
    "model.save_word2vec_format(_vectors_file_, binary =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
